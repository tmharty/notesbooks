{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3060'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./Form-714-csv-files-June-2021/Part 3 Schedule 2 - Planning Area Hourly Demand.csv')\n",
    "respondent_id = pd.read_csv('./Form-714-csv-files-June-2021/Respondent IDs.csv')\n",
    "good_ids = respondent_id['respondent_id'].unique()[3:]\n",
    "df = df[df['respondent_id'].isin(good_ids)]\n",
    "hour_cols = [f'hour{i:02d}' for i in range(1, 25)]\n",
    "df = df.loc[~(df[hour_cols] == 0).any(axis=1)]\n",
    "df['timezone'] = df['timezone'].apply(lambda x: x.strip().upper())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[130128 127461  73773  67919  49870  40665  38843  38732  11033  10197\n",
      "   9469   6201   5476   4734   3312   3087   2559   2557   2191   2188\n",
      "   2152   1658   1649    731    730    730    715    517    363    210\n",
      "      3      2      1      1]\n",
      "['EST' 'CST' 'PST' 'MST' 'CDT' 'CPT' 'PDT' 'EDT' 'DST' 'MDT' 'PPT' 'EPT'\n",
      " 'HST' '2' 'CS' 'AKD' '1' '' 'MS' 'MPP' 'CD' 'EDS' 'AKS' 'MPT' 'EAS' 'CEN'\n",
      " 'CDS' 'AST' 'E' 'ADT' 'CSR' '433' 'CTR' '206']\n",
      "EST 130128\n",
      "CST 127461\n",
      "PST 73773\n",
      "MST 67919\n",
      "CDT 49870\n",
      "CPT 40665\n",
      "PDT 38843\n",
      "EDT 38732\n",
      "DST 11033\n",
      "MDT 10197\n",
      "PPT 9469\n",
      "EPT 6201\n",
      "HST 5476\n",
      "2 4734\n",
      "CS 3312\n",
      "AKD 3087\n",
      "1 2559\n",
      " 2557\n",
      "MS 2191\n",
      "MPP 2188\n",
      "CD 2152\n",
      "EDS 1658\n",
      "AKS 1649\n",
      "MPT 731\n",
      "EAS 730\n",
      "CEN 730\n",
      "CDS 715\n",
      "AST 517\n",
      "E 363\n",
      "ADT 210\n",
      "CSR 3\n",
      "433 2\n",
      "CTR 1\n",
      "206 1\n"
     ]
    }
   ],
   "source": [
    "timezones, counts = np.unique(df['timezone'].values.astype('str'), return_counts=True)\n",
    "argsort = np.argsort(counts)[::-1]\n",
    "print(counts[argsort])\n",
    "print(timezones[argsort])\n",
    "for ii in argsort:\n",
    "    print(timezones[ii], counts[ii])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "timezone_mapping = {\n",
    "    \"EST\": \"Etc/GMT+5\", \n",
    "    \"CST\": \"Etc/GMT+6\", \n",
    "    \"CS\": \"Etc/GMT+6\", \n",
    "    \"PST\": \"Etc/GMT+8\", \n",
    "    \"MST\": \"Etc/GMT+7\", \n",
    "    \"HST\": \"Etc/GMT+10\",\n",
    "    \"AKST\": \"Etc/GMT+9\",\n",
    "    \"EDT\": \"Etc/GMT+4\",\n",
    "    \"CDT\": \"Etc/GMT+5\",\n",
    "    \"CD\": \"Etc/GMT+5\",\n",
    "    \"PDT\": \"Etc/GMT+7\",\n",
    "    \"MDT\": \"Etc/GMT+6\",\n",
    "}\n",
    "df['timezone'] = df['timezone'].apply(timezone_mapping.get)\n",
    "df = df.dropna(subset=['timezone'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[182150 140970 106762  73773  38732   5476]\n",
      "['Etc/GMT+5' 'Etc/GMT+6' 'Etc/GMT+7' 'Etc/GMT+8' 'Etc/GMT+4' 'Etc/GMT+10']\n",
      "Etc/GMT+5 182150\n",
      "Etc/GMT+6 140970\n",
      "Etc/GMT+7 106762\n",
      "Etc/GMT+8 73773\n",
      "Etc/GMT+4 38732\n",
      "Etc/GMT+10 5476\n"
     ]
    }
   ],
   "source": [
    "timezones, counts = np.unique(df['timezone'].values.astype('str'), return_counts=True)\n",
    "argsort = np.argsort(counts)[::-1]\n",
    "print(counts[argsort])\n",
    "print(timezones[argsort])\n",
    "for ii in argsort:\n",
    "    print(timezones[ii], counts[ii])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3370475/2798815225.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.rename(columns=rename_dict, inplace=True)\n",
      "/tmp/ipykernel_3370475/2798815225.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['plan_date'] = pd.to_datetime(df['plan_date'])\n",
      "/tmp/ipykernel_3370475/2798815225.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['plan_date'] = df.apply(\n"
     ]
    }
   ],
   "source": [
    "id_vars = ['respondent_id', 'plan_date', 'timezone']\n",
    "hour_cols = [f'hour{ii:02}' for ii in range(1, 25)]\n",
    "df = df[hour_cols + id_vars]\n",
    "rename_dict = {a_col: int(a_col[-2:]) for a_col in hour_cols}\n",
    "df.rename(columns=rename_dict, inplace=True)\n",
    "df['plan_date'] = pd.to_datetime(df['plan_date'])\n",
    "df['plan_date'] = df.apply(\n",
    "    lambda row: row['plan_date'].tz_localize(\n",
    "        row['timezone']).tz_convert('UTC'),\n",
    "    axis=1\n",
    ")\n",
    "df = pd.melt(df, \n",
    "            id_vars=id_vars,\n",
    "            value_vars=np.arange(1, 25),\n",
    "            var_name='hour',\n",
    "            value_name='load')\n",
    "\n",
    "# Create local datetime\n",
    "df['utc_datetime'] = df['plan_date'] + pd.to_timedelta(df['hour'], unit='h')\n",
    "df = df.dropna(subset=['utc_datetime', 'load'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hour'] = df['utc_datetime'].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = df.groupby(['respondent_id', 'hour'])['load'].mean()\n",
    "stds = df.groupby(['respondent_id', 'hour'])['load'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9866.438238750005"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "means.groupby('hour').std().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "420.74436857807615"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "means.groupby('respondent_id').std().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation\n",
    "def prepare_data(df):\n",
    "    # Feature engineering\n",
    "    df['utc_hour'] = df['utc_datetime'].dt.hour\n",
    "    df['utc_dayofweek'] = df['utc_datetime'].dt.dayofweek\n",
    "    df['utc_month'] = df['utc_datetime'].dt.month\n",
    "    df['utc_year'] = df['utc_datetime'].dt.year\n",
    "    \n",
    "    # Cyclic encoding\n",
    "    df['utc_hour_sin'] = np.sin(2 * np.pi * df['utc_hour'] / 24)\n",
    "    df['utc_hour_cos'] = np.cos(2 * np.pi * df['utc_hour'] / 24)\n",
    "    df['utc_dayofweek_sin'] = np.sin(2 * np.pi * df['utc_dayofweek'] / 7)\n",
    "    df['utc_dayofweek_cos'] = np.cos(2 * np.pi * df['utc_dayofweek'] / 7)\n",
    "    df['utc_month_sin'] = np.sin(2 * np.pi * df['utc_month'] / 12)\n",
    "    df['utc_month_cos'] = np.cos(2 * np.pi * df['utc_month'] / 12)\n",
    "    \n",
    "    # Normalize year\n",
    "    df['utc_year'] = (df['utc_year'] - df['utc_year'].mean()) / df['utc_year'].std()\n",
    "    \n",
    "    # Encode respondents\n",
    "    df['respondent_id'] = df['respondent_id'].astype('category')\n",
    "    df['respondent_idx'] = df['respondent_id'].cat.codes\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_prep = prepare_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Dataset\n",
    "class LoadDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.respondent_idx = torch.LongTensor(df['respondent_idx'].values)\n",
    "        self.features = torch.FloatTensor(df[[\n",
    "            'utc_hour_sin', 'utc_hour_cos',\n",
    "            'utc_dayofweek_sin', 'utc_dayofweek_cos',\n",
    "            'utc_month_sin', 'utc_month_cos',\n",
    "            'utc_year'\n",
    "        ]].values)\n",
    "        self.targets = torch.FloatTensor(df['load'].values)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            self.respondent_idx[idx],\n",
    "            self.features[idx],\n",
    "            self.targets[idx]\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network Model\n",
    "class LoadForecaster(nn.Module):\n",
    "    def __init__(self, num_respondents, embedding_dim=64, hidden_dims=[128, 64, 32, 16]):\n",
    "        super().__init__()\n",
    "        # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.embedding = nn.Embedding(num_respondents, embedding_dim)\n",
    "        # self.embedding.to(device)\n",
    "        self.feature_dim = 7  # 7 temporal features\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embedding_dim + self.feature_dim, hidden_dims[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dims[0], hidden_dims[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dims[1], hidden_dims[2]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dims[2], hidden_dims[3]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dims[3], 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, respondent_idx, features):\n",
    "        # print(respondent_idx.device)\n",
    "        # print(self.embedding.device)\n",
    "        embedded = self.embedding(respondent_idx)\n",
    "        combined = torch.cat([embedded, features], dim=1)\n",
    "        return self.net(combined).squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Setup\n",
    "def train_model(df, num_epochs=20, batch_size=1_000_000):\n",
    "    # Prepare data\n",
    "    # df = prepare_data(df)\n",
    "    # df = df.sort_values('utc_datetime')\n",
    "    \n",
    "    # Split data\n",
    "    train_size = int(0.8 * len(df))\n",
    "    train_df = df.iloc[:train_size]\n",
    "    val_df = df.iloc[train_size:]\n",
    "    \n",
    "    # Create datasets/dataloaders\n",
    "    train_dataset = LoadDataset(train_df)\n",
    "    val_dataset = LoadDataset(val_df)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Initialize model\n",
    "    num_respondents = len(df['respondent_id'].cat.categories)\n",
    "    model = LoadForecaster(num_respondents)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for ridx, features, targets in train_loader:\n",
    "            ridx = ridx.to(device)\n",
    "            features = features.to(device)\n",
    "            targets = targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(ridx, features)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * ridx.size(0)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for ridx, features, targets in val_loader:\n",
    "                ridx = ridx.to(device)\n",
    "                features = features.to(device)\n",
    "                targets = targets.to(device)\n",
    "                outputs = model(ridx, features)\n",
    "                val_loss += criterion(outputs, targets).item() * ridx.size(0)\n",
    "        \n",
    "        # Print statistics\n",
    "        train_loss = train_loss / len(train_dataset)\n",
    "        val_loss = val_loss / len(val_dataset)\n",
    "        print(f'Epoch {epoch+1:2} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3290541/1144664126.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.rename(columns=rename_dict, inplace=True)\n",
      "/tmp/ipykernel_3290541/1144664126.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['plan_date'] = pd.to_datetime(df['plan_date'])\n",
      "/tmp/ipykernel_3290541/1144664126.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['plan_date'] = df.apply(\n"
     ]
    }
   ],
   "source": [
    "df_prep = prepare_data(df)\n",
    "df_prep = df_prep.sort_values('utc_datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 | Train Loss: 144180872.7178 | Val Loss: 164716475.0125\n",
      "Epoch  2 | Train Loss: 144179126.6507 | Val Loss: 164712984.2106\n",
      "Epoch  3 | Train Loss: 144172274.9683 | Val Loss: 164698673.2960\n",
      "Epoch  4 | Train Loss: 144145532.8302 | Val Loss: 164646598.2056\n",
      "Epoch  5 | Train Loss: 144053899.7229 | Val Loss: 164480464.7528\n",
      "Epoch  6 | Train Loss: 143778018.4159 | Val Loss: 164015687.8396\n",
      "Epoch  7 | Train Loss: 143042406.7607 | Val Loss: 162840644.1520\n",
      "Epoch  8 | Train Loss: 141275213.6066 | Val Loss: 160155465.6589\n",
      "Epoch  9 | Train Loss: 137460103.2774 | Val Loss: 154628098.4353\n",
      "Epoch 10 | Train Loss: 130113458.0864 | Val Loss: 144501121.2649\n",
      "Epoch 11 | Train Loss: 117893168.5612 | Val Loss: 128462440.3824\n",
      "Epoch 12 | Train Loss: 100929617.7755 | Val Loss: 107436421.4132\n",
      "Epoch 13 | Train Loss: 82393959.6603 | Val Loss: 85979357.5755\n",
      "Epoch 14 | Train Loss: 66505885.7505 | Val Loss: 69083856.0508\n",
      "Epoch 15 | Train Loss: 53770160.3376 | Val Loss: 56596217.6811\n",
      "Epoch 16 | Train Loss: 42839246.9024 | Val Loss: 46502006.7243\n",
      "Epoch 17 | Train Loss: 33646936.4682 | Val Loss: 37554726.1045\n",
      "Epoch 18 | Train Loss: 25981355.7373 | Val Loss: 29908236.3752\n",
      "Epoch 19 | Train Loss: 19769029.9675 | Val Loss: 23699735.7150\n",
      "Epoch 20 | Train Loss: 14902300.7870 | Val Loss: 18793632.3473\n"
     ]
    }
   ],
   "source": [
    "model = train_model(df_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), 'load_forecaster_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
