{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3060'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./Form-714-csv-files-June-2021/Part 3 Schedule 2 - Planning Area Hourly Demand.csv')\n",
    "respondent_id = pd.read_csv('./Form-714-csv-files-June-2021/Respondent IDs.csv')\n",
    "good_ids = respondent_id['respondent_id'].unique()[3:]\n",
    "df = df[df['respondent_id'].isin(good_ids)]\n",
    "hour_cols = [f'hour{i:02d}' for i in range(1, 25)]\n",
    "df = df.loc[~(df[hour_cols] == 0).any(axis=1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def previous_day_index(respondent_data, x):\n",
    "    respondent_data['plan_date'] == x - pd.Timedelta('1d')\n",
    "    a_index = respondent_data.index[respondent_data['plan_date'] == x - pd.Timedelta('1d')]\n",
    "    if len(a_index) == 0:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return a_index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation\n",
    "def prepare_data(df):\n",
    "    # Keep hourly columns as features\n",
    "    hour_cols = [f'hour{i:02d}' for i in range(1, 25)]\n",
    "    \n",
    "    # Convert date and extract features\n",
    "    df['plan_date'] = pd.to_datetime(df['plan_date'])\n",
    "    df['year'] = df['plan_date'].dt.year\n",
    "    df['month'] = df['plan_date'].dt.month\n",
    "    df['day_of_week'] = df['plan_date'].dt.dayofweek\n",
    "    \n",
    "    # Cyclic encoding for temporal features\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "    df['day_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
    "    df['day_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
    "    \n",
    "    # Normalize year\n",
    "    df['year'] = (df['year'] - df['year'].mean()) / df['year'].std()\n",
    "    \n",
    "    # Encode respondents\n",
    "    df['respondent_id'] = df['respondent_id'].astype('category')\n",
    "    df['respondent_idx'] = df['respondent_id'].cat.codes\n",
    "    \n",
    "    # Normalize load values per respondent\n",
    "    for col in hour_cols:\n",
    "        # save original data\n",
    "        df['og_' + col] = df[col].copy()\n",
    "        # Normalize load values per respondent\n",
    "        df[col] = df.groupby('respondent_id')[col].transform(\n",
    "            lambda x: (x - x.mean()) / x.std()\n",
    "        )\n",
    "    df = df.reset_index()\n",
    "    df['prev_day_index'] = np.nan\n",
    "    for a_id in df['respondent_id'].unique():\n",
    "        respondent_data = df.loc[df['respondent_id'] == a_id]\n",
    "        respondent_data.loc[:, 'prev_day_index'] = (\n",
    "            respondent_data['plan_date'].apply(\n",
    "                partial(previous_day_index, respondent_data)))\n",
    "        df.loc[respondent_data.index, 'prev_day_index'] = respondent_data['prev_day_index']\n",
    "    prev_hour_cols = ['prev_'+ a_hour for a_hour in hour_cols]\n",
    "    df[prev_hour_cols] = np.nan\n",
    "    with_prev = df['prev_day_index'].notna()\n",
    "    df.loc[with_prev, prev_hour_cols] = (\n",
    "        df.loc[df['prev_day_index'].loc[with_prev], hour_cols].values\n",
    "    )\n",
    "    df = df.dropna()\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3794540/3322726924.py:30: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  df[col] = df.groupby('respondent_id')[col].transform(\n",
      "/tmp/ipykernel_3794540/3322726924.py:30: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  df[col] = df.groupby('respondent_id')[col].transform(\n",
      "/tmp/ipykernel_3794540/3322726924.py:30: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  df[col] = df.groupby('respondent_id')[col].transform(\n",
      "/tmp/ipykernel_3794540/3322726924.py:30: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  df[col] = df.groupby('respondent_id')[col].transform(\n",
      "/tmp/ipykernel_3794540/3322726924.py:30: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  df[col] = df.groupby('respondent_id')[col].transform(\n",
      "/tmp/ipykernel_3794540/3322726924.py:30: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  df[col] = df.groupby('respondent_id')[col].transform(\n",
      "/tmp/ipykernel_3794540/3322726924.py:30: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  df[col] = df.groupby('respondent_id')[col].transform(\n",
      "/tmp/ipykernel_3794540/3322726924.py:30: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  df[col] = df.groupby('respondent_id')[col].transform(\n",
      "/tmp/ipykernel_3794540/3322726924.py:30: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  df[col] = df.groupby('respondent_id')[col].transform(\n",
      "/tmp/ipykernel_3794540/3322726924.py:30: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  df[col] = df.groupby('respondent_id')[col].transform(\n",
      "/tmp/ipykernel_3794540/3322726924.py:30: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  df[col] = df.groupby('respondent_id')[col].transform(\n",
      "/tmp/ipykernel_3794540/3322726924.py:30: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  df[col] = df.groupby('respondent_id')[col].transform(\n",
      "/tmp/ipykernel_3794540/3322726924.py:30: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  df[col] = df.groupby('respondent_id')[col].transform(\n",
      "/tmp/ipykernel_3794540/3322726924.py:30: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  df[col] = df.groupby('respondent_id')[col].transform(\n",
      "/tmp/ipykernel_3794540/3322726924.py:30: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  df[col] = df.groupby('respondent_id')[col].transform(\n",
      "/tmp/ipykernel_3794540/3322726924.py:30: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  df[col] = df.groupby('respondent_id')[col].transform(\n",
      "/tmp/ipykernel_3794540/3322726924.py:30: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  df[col] = df.groupby('respondent_id')[col].transform(\n",
      "/tmp/ipykernel_3794540/3322726924.py:30: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  df[col] = df.groupby('respondent_id')[col].transform(\n",
      "/tmp/ipykernel_3794540/3322726924.py:30: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  df[col] = df.groupby('respondent_id')[col].transform(\n",
      "/tmp/ipykernel_3794540/3322726924.py:30: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  df[col] = df.groupby('respondent_id')[col].transform(\n",
      "/tmp/ipykernel_3794540/3322726924.py:30: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  df[col] = df.groupby('respondent_id')[col].transform(\n",
      "/tmp/ipykernel_3794540/3322726924.py:30: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  df[col] = df.groupby('respondent_id')[col].transform(\n",
      "/tmp/ipykernel_3794540/3322726924.py:30: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  df[col] = df.groupby('respondent_id')[col].transform(\n",
      "/tmp/ipykernel_3794540/3322726924.py:30: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  df[col] = df.groupby('respondent_id')[col].transform(\n"
     ]
    }
   ],
   "source": [
    "df_prep = prepare_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Dataset\n",
    "class LoadDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.respondent_idx = torch.LongTensor(df['respondent_idx'].values)\n",
    "        self.prev_targets = torch.FloatTensor(\n",
    "            df[[f'prev_hour{i:02d}' for i in range(1, 25)]].values)\n",
    "        self.temporal = torch.FloatTensor(df[['year', 'month_sin', 'month_cos', 'day_sin', 'day_cos']].values)\n",
    "        self.targets = torch.FloatTensor(\n",
    "            df[[f'hour{i:02d}' for i in range(1, 25)]].values)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            self.respondent_idx[idx],\n",
    "            self.prev_targets[idx],\n",
    "            self.temporal[idx],\n",
    "            self.targets[idx]\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network Model\n",
    "class LoadForecaster(nn.Module):\n",
    "    def __init__(self, num_respondents, embedding_dim=64, hidden_dims=[512, 512, 256, 128, 64l]):\n",
    "        super().__init__()\n",
    "        # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.embedding = nn.Embedding(num_respondents, embedding_dim)\n",
    "        # self.embedding.to(device)\n",
    "        self.feature_dim = 5  + 24 # 5 temporal features 24 previous obs\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embedding_dim + self.feature_dim, hidden_dims[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dims[0], hidden_dims[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dims[1], hidden_dims[2]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dims[2], hidden_dims[3]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dims[3], 24)\n",
    "        )\n",
    "        \n",
    "    def forward(self, respondent_idx, temporal, prev_targets):\n",
    "        embedded = self.embedding(respondent_idx)\n",
    "        combined = torch.cat([embedded, temporal, prev_targets], dim=1)\n",
    "        return self.net(combined).squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Setup\n",
    "def train_model(df, num_epochs=100, batch_size=2**15):\n",
    "    # Prepare data\n",
    "    # df = prepare_data(df)\n",
    "    \n",
    "    # Split data\n",
    "    train_size = int(0.8 * len(df))\n",
    "    train_df = df.iloc[:train_size]\n",
    "    val_df = df.iloc[train_size:]\n",
    "    \n",
    "    # Create datasets/dataloaders\n",
    "    train_dataset = LoadDataset(train_df)\n",
    "    val_dataset = LoadDataset(val_df)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Initialize model\n",
    "    num_respondents = len(df['respondent_id'].cat.categories)\n",
    "    model = LoadForecaster(num_respondents)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for ridx, prev_targets, temporal, targets in train_loader:\n",
    "            ridx = ridx.to(device)\n",
    "            prev_targets = prev_targets.to(device)\n",
    "            temporal = temporal.to(device)\n",
    "            targets = targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(ridx, temporal, prev_targets)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * ridx.size(0)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for ridx, prev_targets, temporal, targets in val_loader:\n",
    "                ridx = ridx.to(device)\n",
    "                prev_targets = prev_targets.to(device)\n",
    "                temporal = temporal.to(device)\n",
    "                targets = targets.to(device)\n",
    "                outputs = model(ridx, temporal, prev_targets)\n",
    "                val_loss += criterion(outputs, targets).item() * ridx.size(0)\n",
    "        \n",
    "        # Print statistics\n",
    "        train_loss = train_loss / len(train_dataset)\n",
    "        val_loss = val_loss / len(val_dataset)\n",
    "        print(f'Epoch {epoch+1:2} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 | Train Loss: 0.5447 | Val Loss: 0.3019\n",
      "Epoch  2 | Train Loss: 0.2265 | Val Loss: 0.2053\n",
      "Epoch  3 | Train Loss: 0.1740 | Val Loss: 0.1827\n",
      "Epoch  4 | Train Loss: 0.1565 | Val Loss: 0.1717\n",
      "Epoch  5 | Train Loss: 0.1461 | Val Loss: 0.1575\n",
      "Epoch  6 | Train Loss: 0.1369 | Val Loss: 0.1511\n",
      "Epoch  7 | Train Loss: 0.1291 | Val Loss: 0.1461\n",
      "Epoch  8 | Train Loss: 0.1220 | Val Loss: 0.1399\n",
      "Epoch  9 | Train Loss: 0.1155 | Val Loss: 0.1352\n",
      "Epoch 10 | Train Loss: 0.1260 | Val Loss: 0.1459\n",
      "Epoch 11 | Train Loss: 0.1149 | Val Loss: 0.1337\n",
      "Epoch 12 | Train Loss: 0.1081 | Val Loss: 0.1294\n",
      "Epoch 13 | Train Loss: 0.1053 | Val Loss: 0.1272\n",
      "Epoch 14 | Train Loss: 0.1034 | Val Loss: 0.1260\n",
      "Epoch 15 | Train Loss: 0.1019 | Val Loss: 0.1240\n",
      "Epoch 16 | Train Loss: 0.1006 | Val Loss: 0.1242\n",
      "Epoch 17 | Train Loss: 0.0994 | Val Loss: 0.1215\n",
      "Epoch 18 | Train Loss: 0.0983 | Val Loss: 0.1229\n",
      "Epoch 19 | Train Loss: 0.0990 | Val Loss: 0.1200\n",
      "Epoch 20 | Train Loss: 0.0964 | Val Loss: 0.1193\n",
      "Epoch 21 | Train Loss: 0.0961 | Val Loss: 0.1227\n",
      "Epoch 22 | Train Loss: 0.0967 | Val Loss: 0.1213\n",
      "Epoch 23 | Train Loss: 0.0951 | Val Loss: 0.1178\n",
      "Epoch 24 | Train Loss: 0.0937 | Val Loss: 0.1183\n",
      "Epoch 25 | Train Loss: 0.0933 | Val Loss: 0.1172\n",
      "Epoch 26 | Train Loss: 0.0925 | Val Loss: 0.1178\n",
      "Epoch 27 | Train Loss: 0.0984 | Val Loss: 0.1173\n",
      "Epoch 28 | Train Loss: 0.0927 | Val Loss: 0.1155\n",
      "Epoch 29 | Train Loss: 0.0910 | Val Loss: 0.1166\n",
      "Epoch 30 | Train Loss: 0.0908 | Val Loss: 0.1151\n",
      "Epoch 31 | Train Loss: 0.0901 | Val Loss: 0.1152\n",
      "Epoch 32 | Train Loss: 0.0902 | Val Loss: 0.1144\n",
      "Epoch 33 | Train Loss: 0.0893 | Val Loss: 0.1162\n",
      "Epoch 34 | Train Loss: 0.0915 | Val Loss: 0.1142\n",
      "Epoch 35 | Train Loss: 0.0891 | Val Loss: 0.1150\n",
      "Epoch 36 | Train Loss: 0.0882 | Val Loss: 0.1139\n",
      "Epoch 37 | Train Loss: 0.0880 | Val Loss: 0.1149\n",
      "Epoch 38 | Train Loss: 0.0917 | Val Loss: 0.1132\n",
      "Epoch 39 | Train Loss: 0.0884 | Val Loss: 0.1140\n",
      "Epoch 40 | Train Loss: 0.0873 | Val Loss: 0.1129\n",
      "Epoch 41 | Train Loss: 0.0871 | Val Loss: 0.1146\n",
      "Epoch 42 | Train Loss: 0.0869 | Val Loss: 0.1136\n",
      "Epoch 43 | Train Loss: 0.0884 | Val Loss: 0.1130\n",
      "Epoch 44 | Train Loss: 0.0869 | Val Loss: 0.1132\n",
      "Epoch 45 | Train Loss: 0.0868 | Val Loss: 0.1137\n",
      "Epoch 46 | Train Loss: 0.0862 | Val Loss: 0.1151\n",
      "Epoch 47 | Train Loss: 0.0865 | Val Loss: 0.1148\n",
      "Epoch 48 | Train Loss: 0.0856 | Val Loss: 0.1128\n",
      "Epoch 49 | Train Loss: 0.0854 | Val Loss: 0.1129\n",
      "Epoch 50 | Train Loss: 0.0861 | Val Loss: 0.1124\n",
      "Epoch 51 | Train Loss: 0.0855 | Val Loss: 0.1125\n",
      "Epoch 52 | Train Loss: 0.0845 | Val Loss: 0.1134\n",
      "Epoch 53 | Train Loss: 0.0843 | Val Loss: 0.1125\n",
      "Epoch 54 | Train Loss: 0.0848 | Val Loss: 0.1130\n",
      "Epoch 55 | Train Loss: 0.0843 | Val Loss: 0.1128\n",
      "Epoch 56 | Train Loss: 0.0860 | Val Loss: 0.1105\n",
      "Epoch 57 | Train Loss: 0.0841 | Val Loss: 0.1123\n",
      "Epoch 58 | Train Loss: 0.0837 | Val Loss: 0.1141\n",
      "Epoch 59 | Train Loss: 0.0834 | Val Loss: 0.1117\n",
      "Epoch 60 | Train Loss: 0.0840 | Val Loss: 0.1122\n",
      "Epoch 61 | Train Loss: 0.0832 | Val Loss: 0.1121\n",
      "Epoch 62 | Train Loss: 0.0843 | Val Loss: 0.1124\n",
      "Epoch 63 | Train Loss: 0.0828 | Val Loss: 0.1108\n",
      "Epoch 64 | Train Loss: 0.0835 | Val Loss: 0.1115\n",
      "Epoch 65 | Train Loss: 0.0826 | Val Loss: 0.1121\n",
      "Epoch 66 | Train Loss: 0.0825 | Val Loss: 0.1127\n",
      "Epoch 67 | Train Loss: 0.0827 | Val Loss: 0.1122\n",
      "Epoch 68 | Train Loss: 0.0824 | Val Loss: 0.1103\n",
      "Epoch 69 | Train Loss: 0.0847 | Val Loss: 0.1113\n",
      "Epoch 70 | Train Loss: 0.0820 | Val Loss: 0.1103\n",
      "Epoch 71 | Train Loss: 0.0815 | Val Loss: 0.1130\n",
      "Epoch 72 | Train Loss: 0.0819 | Val Loss: 0.1148\n",
      "Epoch 73 | Train Loss: 0.0825 | Val Loss: 0.1100\n",
      "Epoch 74 | Train Loss: 0.0815 | Val Loss: 0.1105\n",
      "Epoch 75 | Train Loss: 0.0814 | Val Loss: 0.1101\n",
      "Epoch 76 | Train Loss: 0.0809 | Val Loss: 0.1133\n",
      "Epoch 77 | Train Loss: 0.0814 | Val Loss: 0.1126\n",
      "Epoch 78 | Train Loss: 0.0811 | Val Loss: 0.1101\n",
      "Epoch 79 | Train Loss: 0.0825 | Val Loss: 0.1145\n",
      "Epoch 80 | Train Loss: 0.0817 | Val Loss: 0.1115\n",
      "Epoch 81 | Train Loss: 0.0806 | Val Loss: 0.1104\n",
      "Epoch 82 | Train Loss: 0.0806 | Val Loss: 0.1117\n",
      "Epoch 83 | Train Loss: 0.0813 | Val Loss: 0.1096\n",
      "Epoch 84 | Train Loss: 0.0802 | Val Loss: 0.1100\n",
      "Epoch 85 | Train Loss: 0.0796 | Val Loss: 0.1115\n",
      "Epoch 86 | Train Loss: 0.0814 | Val Loss: 0.1096\n",
      "Epoch 87 | Train Loss: 0.0798 | Val Loss: 0.1110\n",
      "Epoch 88 | Train Loss: 0.0797 | Val Loss: 0.1100\n",
      "Epoch 89 | Train Loss: 0.0811 | Val Loss: 0.1111\n",
      "Epoch 90 | Train Loss: 0.0796 | Val Loss: 0.1135\n",
      "Epoch 91 | Train Loss: 0.0798 | Val Loss: 0.1107\n",
      "Epoch 92 | Train Loss: 0.0793 | Val Loss: 0.1103\n",
      "Epoch 93 | Train Loss: 0.0795 | Val Loss: 0.1118\n",
      "Epoch 94 | Train Loss: 0.0804 | Val Loss: 0.1094\n",
      "Epoch 95 | Train Loss: 0.0798 | Val Loss: 0.1100\n",
      "Epoch 96 | Train Loss: 0.0793 | Val Loss: 0.1109\n",
      "Epoch 97 | Train Loss: 0.0791 | Val Loss: 0.1101\n",
      "Epoch 98 | Train Loss: 0.0801 | Val Loss: 0.1100\n",
      "Epoch 99 | Train Loss: 0.0787 | Val Loss: 0.1102\n",
      "Epoch 100 | Train Loss: 0.0787 | Val Loss: 0.1105\n"
     ]
    }
   ],
   "source": [
    "model = train_model(df_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'load_forecaster_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
