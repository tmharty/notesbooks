{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3060'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./Form-714-csv-files-June-2021/Part 3 Schedule 2 - Planning Area Hourly Demand.csv')\n",
    "respondent_id = pd.read_csv('./Form-714-csv-files-June-2021/Respondent IDs.csv')\n",
    "good_ids = respondent_id['respondent_id'].unique()[3:]\n",
    "df = df[df['respondent_id'].isin(good_ids)]\n",
    "hour_cols = [f'hour{i:02d}' for i in range(1, 25)]\n",
    "df = df.loc[~(df[hour_cols] == 0).any(axis=1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_vae_data(df):\n",
    "    # Keep hourly columns as features\n",
    "    hour_cols = [f'hour{i:02d}' for i in range(1, 25)]\n",
    "    \n",
    "    # Convert date and extract features\n",
    "    df['plan_date'] = pd.to_datetime(df['plan_date'])\n",
    "    df['year'] = df['plan_date'].dt.year\n",
    "    df['month'] = df['plan_date'].dt.month\n",
    "    df['day_of_week'] = df['plan_date'].dt.dayofweek\n",
    "    \n",
    "    # Cyclic encoding for temporal features\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "    df['day_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
    "    df['day_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
    "    \n",
    "    # Normalize year\n",
    "    df['year'] = (df['year'] - df['year'].mean()) / df['year'].std()\n",
    "    \n",
    "    # Encode respondents\n",
    "    df['respondent_id'] = df['respondent_id'].astype('category')\n",
    "    df['respondent_idx'] = df['respondent_id'].cat.codes\n",
    "    \n",
    "    # Normalize load values per respondent\n",
    "    for col in hour_cols:\n",
    "        df[col] = df.groupby('respondent_id')[col].transform(\n",
    "            lambda x: (x - x.mean()) / x.std()\n",
    "        )\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE Dataset\n",
    "class VAEDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.loads = torch.FloatTensor(df[[f'hour{i:02d}' for i in range(1, 25)]].values)\n",
    "        self.respondents = torch.LongTensor(df['respondent_idx'].values)\n",
    "        self.temporal = torch.FloatTensor(df[['year', 'month_sin', 'month_cos', 'day_sin', 'day_cos']].values)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.loads)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.loads[idx], self.respondents[idx], self.temporal[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# VAE Model\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, num_respondents, temporal_dim=5, load_dim=24, \n",
    "                 embed_dim=10, hidden_dim=256, latent_dim=32):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embed = nn.Embedding(num_respondents, embed_dim)\n",
    "        self.temporal_dim = temporal_dim\n",
    "        self.load_dim = load_dim\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(load_dim + embed_dim + temporal_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim//2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(hidden_dim//2, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim//2, latent_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim + embed_dim + temporal_dim, hidden_dim//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim//2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, load_dim)\n",
    "        )\n",
    "    \n",
    "    def encode(self, x, r, t):\n",
    "        embedded = self.embed(r)\n",
    "        combined = torch.cat([x, embedded, t], dim=1)\n",
    "        h = self.encoder(combined)\n",
    "        return self.fc_mu(h), self.fc_logvar(h)\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "    \n",
    "    def decode(self, z, r, t):\n",
    "        embedded = self.embed(r)\n",
    "        combined = torch.cat([z, embedded, t], dim=1)\n",
    "        return self.decoder(combined)\n",
    "    \n",
    "    def forward(self, x, r, t):\n",
    "        mu, logvar = self.encode(x, r, t)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z, r, t), mu, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training Function\n",
    "def train_vae(df, num_epochs=50, batch_size=256):\n",
    "    df = prepare_vae_data(df)\n",
    "    dataset = VAEDataset(df)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    num_respondents = len(df['respondent_id'].cat.categories)\n",
    "    model = VAE(num_respondents)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for loads, respondents, temporal in loader:\n",
    "            optimizer.zero_grad()\n",
    "            recon, mu, logvar = model(loads, respondents, temporal)\n",
    "            \n",
    "            # Reconstruction loss + KL divergence\n",
    "            recon_loss = nn.functional.mse_loss(recon, loads, reduction='sum')\n",
    "            kl_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "            \n",
    "            loss = recon_loss + kl_div\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        print(f'Epoch {epoch+1}, Loss: {train_loss/len(dataset):.4f}')\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3375966/2264905904.py:26: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  df[col] = df.groupby('respondent_id')[col].transform(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 4.7170\n",
      "Epoch 2, Loss: 3.6255\n",
      "Epoch 3, Loss: 3.3675\n",
      "Epoch 4, Loss: 3.2561\n",
      "Epoch 5, Loss: 3.1849\n",
      "Epoch 6, Loss: 3.1356\n",
      "Epoch 7, Loss: 3.0983\n",
      "Epoch 8, Loss: 3.0736\n",
      "Epoch 9, Loss: 3.0457\n",
      "Epoch 10, Loss: 3.0234\n",
      "Epoch 11, Loss: 3.0053\n",
      "Epoch 12, Loss: 2.9923\n",
      "Epoch 13, Loss: 2.9792\n",
      "Epoch 14, Loss: 2.9624\n",
      "Epoch 15, Loss: 2.9532\n",
      "Epoch 16, Loss: 2.9421\n",
      "Epoch 17, Loss: 2.9334\n",
      "Epoch 18, Loss: 2.9248\n",
      "Epoch 19, Loss: 2.9152\n",
      "Epoch 20, Loss: 2.9031\n",
      "Epoch 21, Loss: 2.8998\n",
      "Epoch 22, Loss: 2.8911\n",
      "Epoch 23, Loss: 2.8857\n",
      "Epoch 24, Loss: 2.8797\n",
      "Epoch 25, Loss: 2.8743\n",
      "Epoch 26, Loss: 2.8685\n",
      "Epoch 27, Loss: 2.8596\n",
      "Epoch 28, Loss: 2.8607\n",
      "Epoch 29, Loss: 2.8516\n",
      "Epoch 30, Loss: 2.8493\n",
      "Epoch 31, Loss: 2.8450\n",
      "Epoch 32, Loss: 2.8404\n",
      "Epoch 33, Loss: 2.8380\n",
      "Epoch 34, Loss: 2.8347\n",
      "Epoch 35, Loss: 2.8297\n",
      "Epoch 36, Loss: 2.8265\n",
      "Epoch 37, Loss: 2.8231\n",
      "Epoch 38, Loss: 2.8222\n",
      "Epoch 39, Loss: 2.8168\n",
      "Epoch 40, Loss: 2.8104\n",
      "Epoch 41, Loss: 2.8113\n",
      "Epoch 42, Loss: 2.8093\n",
      "Epoch 43, Loss: 2.8089\n",
      "Epoch 44, Loss: 2.8029\n",
      "Epoch 45, Loss: 2.8019\n",
      "Epoch 46, Loss: 2.7977\n",
      "Epoch 47, Loss: 2.7965\n",
      "Epoch 48, Loss: 2.7943\n",
      "Epoch 49, Loss: 2.7937\n",
      "Epoch 50, Loss: 2.7878\n"
     ]
    }
   ],
   "source": [
    "vae_model = train_vae(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(vae_model.state_dict(), 'load_vae_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
